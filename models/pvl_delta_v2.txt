model {
  # pvl_delta_v2.txt
  # ----------------
  # Fixed version of PVL-Delta with proper parameter bounds.
  # Changes from v1:
  #   - A parameter bounded to [0,1] (was unbounded)
  #   - Ev initialized to 0 (was random prior)
  #   - More informative group-level priors

  # Group-level priors
  mu_w ~ dnorm(1, 1)T(0,)          # Loss aversion, typically > 1
  mu_A ~ dnorm(0.5, 1)T(0,1)       # Shape parameter, bounded [0,1]
  mu_theta ~ dnorm(1, 1)T(0,)      # Consistency, positive
  mu_a ~ dnorm(0.5, 1)T(0,1)       # Learning rate, bounded [0,1]

  # Precision priors
  lambda_w ~ dgamma(1, 0.5)
  lambda_A ~ dgamma(1, 0.5)
  lambda_theta ~ dgamma(1, 0.5)
  lambda_a ~ dgamma(1, 0.5)

  for (s in 1:nsubs) {

    # Subject-level parameters with proper truncation
    w[s] ~ dnorm(mu_w, lambda_w)T(0,)
    A[s] ~ dnorm(mu_A, lambda_A)T(0,1)
    theta[s] ~ dnorm(mu_theta, lambda_theta)T(0,)
    a[s] ~ dnorm(mu_a, lambda_a)T(0,1)

    # Fixed initialization (matches hBayesDM)
    Ev[s,1,1] <- 0
    Ev[s,1,2] <- 0
    Ev[s,1,3] <- 0
    Ev[s,1,4] <- 0

    for (t in 2:ntrials[s]) {

      for (d in 1:4) {
        # Prospect theory utility function
        # Must use abs() because X can be 0 or negative, and A is fractional
        u[s,t,d] <- ifelse(X[s,t-1] < 0,
                           -w[s] * pow(abs(X[s,t-1]) + 0.0001, A[s]),
                           pow(abs(X[s,t-1]) + 0.0001, A[s]))

        # Delta learning rule: only update chosen deck
        Ev_update[s,t,d] <- Ev[s,t-1,d] + a[s] * (u[s,t,d] - Ev[s,t-1,d])
        Ev[s,t,d] <- ifelse(x[s,t-1] == d, Ev_update[s,t,d], Ev[s,t-1,d])

        # Softmax (step 1)
        exp_p[s,t,d] <- exp(theta[s] * Ev[s,t,d])
      }

      # Softmax (step 2)
      for (d in 1:4) {
        p[s,t,d] <- exp_p[s,t,d] / sum(exp_p[s,t,])
      }

      # Choice
      x[s,t] ~ dcat(p[s,t,])
    }
  }
}
