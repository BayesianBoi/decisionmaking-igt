model {
  # ===========================================================================
  # ORL Model for Iowa Gambling Task
  # ===========================================================================
  #
  # Reference: Haines, N., Vassileva, J., & Ahn, W.Y. (2018). The Outcome-
  #            Representation Learning model: A novel reinforcement learning
  #            model of the Iowa Gambling Task. Cognitive Science.
  #
  # The ORL model maintains three psychological representations that combine
  # to guide choice. Expected value (EV) tracks outcome magnitudes, expected
  # frequency (EF) tracks win/loss frequency, and perseverance (Pers) tracks
  # recent choice history. This structure allows the model to capture distinct
  # clinical phenotypes.
  #
  # Parameters (5 total):
  #   A_rew  - Reward learning rate [0,1]: Speed of updating after gains
  #   A_pun  - Punishment learning rate [0,1]: Speed of updating after losses
  #   K      - Perseverance decay [0,5]: How fast choice tendencies fade
  #   beta_F - Frequency weight [-5,5]: Influence of win frequency on choice
  #   beta_P - Perseverance weight [-5,5]: Influence of recent choices
  #
  # Note: Choice consistency (theta) is fixed at 1 for identifiability
  #       (Haines et al., 2018)
  #
  # ===========================================================================

  # ---------------------------------------------------------------------------
  # Group-level priors (hyperpriors)
  # ---------------------------------------------------------------------------

  # Reward learning rate: Beta(2, 2) centred at 0.5
  # Haines et al. (2018) found typical values 0.01-0.30
  mu_Arew ~ dbeta(2, 2)

  # Punishment learning rate: Beta(2, 2) centred at 0.5
  # May differ from A_rew in clinical populations
  mu_Apun ~ dbeta(2, 2)

  # Perseverance decay: Normal(1, 1) truncated at 0 and 5
  # Higher values = faster decay of choice tendencies
  # Haines et al. (2018) found typical values around 1-3
  mu_K ~ dnorm(1, 1)T(0, 5)

  # Frequency weight: Normal(0, 0.5) truncated at -5 and 5
  # Positive = prefer decks that win often
  # Negative = prefer decks that win rarely
  # Centred at zero (agnostic)
  mu_betaF ~ dnorm(0, 0.5)T(-5, 5)

  # Perseverance weight: Normal(0, 0.5) truncated at -5 and 5
  # Positive = repeat recent choices
  # Negative = switch away from recent choices
  # Centred at zero (agnostic)
  mu_betaP ~ dnorm(0, 0.5)T(-5, 5)

  # Group-level standard deviations
  sigma_Arew ~ dunif(0, 0.5)
  sigma_Apun ~ dunif(0, 0.5)
  sigma_K ~ dunif(0, 1.5)
  sigma_betaF ~ dunif(0, 2)
  sigma_betaP ~ dunif(0, 2)

  # ---------------------------------------------------------------------------
  # Subject-level parameters
  # ---------------------------------------------------------------------------

  for (s in 1:N) {
    # Learning rates use beta distribution
    Arew[s] ~ dbeta(mu_Arew * (1/sigma_Arew^2 - 1),
                    (1-mu_Arew) * (1/sigma_Arew^2 - 1))T(0.01, 0.99)
    Apun[s] ~ dbeta(mu_Apun * (1/sigma_Apun^2 - 1),
                    (1-mu_Apun) * (1/sigma_Apun^2 - 1))T(0.01, 0.99)

    # Other parameters use truncated normal
    K[s] ~ dnorm(mu_K, 1/sigma_K^2)T(0.01, 5)
    betaF[s] ~ dnorm(mu_betaF, 1/sigma_betaF^2)T(-5, 5)
    betaP[s] ~ dnorm(mu_betaP, 1/sigma_betaP^2)T(-5, 5)
  }

  # ---------------------------------------------------------------------------
  # Trial-level model
  # ---------------------------------------------------------------------------

  for (s in 1:N) {

    # Initialize all representations to zero
    for (d in 1:4) {
      ev[s, 1, d] <- 0
      ef[s, 1, d] <- 0
      pers[s, 1, d] <- 0
    }

    for (t in 1:Tsubj[s]) {

      # Compute combined utility for each deck
      for (d in 1:4) {
        util[s, t, d] <- ev[s, t, d] + betaF[s] * ef[s, t, d] + betaP[s] * pers[s, t, d]
      }

      # Softmax choice probabilities (theta fixed at 1)
      for (d in 1:4) {
        exp_util[s, t, d] <- exp(util[s, t, d])
      }

      for (d in 1:4) {
        p[s, t, d] <- exp_util[s, t, d] / sum(exp_util[s, t, ])
      }

      # Likelihood of observed choice
      choice[s, t] ~ dcat(p[s, t, ])

      # Sign-coded outcome for frequency updating
      # +1 for gains, -1 for losses, 0 for neutral
      sign_outcome[s, t] <- ifelse(outcome[s, t] > 0, 1,
                                   ifelse(outcome[s, t] < 0, -1, 0))

      # Update representations for each deck
      for (d in 1:4) {

        # Expected value update (asymmetric learning rates)
        ev[s, t+1, d] <- ifelse(equals(d, choice[s, t]) == 1,
                                ifelse(outcome[s, t] >= 0,
                                       ev[s, t, d] + Arew[s] * (outcome[s, t] - ev[s, t, d]),
                                       ev[s, t, d] + Apun[s] * (outcome[s, t] - ev[s, t, d])),
                                ev[s, t, d])

        # Expected frequency update
        # Chosen deck: update toward sign of outcome
        # Unchosen decks: update toward opposite sign (counterfactual)
        ef[s, t+1, d] <- ifelse(equals(d, choice[s, t]) == 1,
                                ifelse(outcome[s, t] >= 0,
                                       ef[s, t, d] + Arew[s] * (sign_outcome[s, t] - ef[s, t, d]),
                                       ef[s, t, d] + Apun[s] * (sign_outcome[s, t] - ef[s, t, d])),
                                ifelse(outcome[s, t] >= 0,
                                       ef[s, t, d] + Apun[s] * (-sign_outcome[s, t] / 3 - ef[s, t, d]),
                                       ef[s, t, d] + Arew[s] * (-sign_outcome[s, t] / 3 - ef[s, t, d])))

        # Perseverance update
        # Chosen deck: set to 1 then decay
        # Unchosen decks: decay only
        pers[s, t+1, d] <- ifelse(equals(d, choice[s, t]) == 1,
                                  1 / (1 + K[s]),
                                  pers[s, t, d] / (1 + K[s]))
      }
    }
  }
}
