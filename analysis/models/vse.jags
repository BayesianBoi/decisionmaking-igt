model {
  # ===========================================================================
  # VSE Model for Iowa Gambling Task
  # ===========================================================================
  #
  # Reference: Worthy, D.A., Pang, B., & Byrne, K.A. (2013). Decomposing the
  #            roles of perseveration and expected value representation in
  #            models of the Iowa gambling task. Frontiers in Psychology.
  #
  # The VSE (Value plus Sequential Exploration) model extends PVL-Delta by
  # adding a perseverance mechanism that tracks recent choice tendencies
  # independent of outcome value.
  #
  # Parameters (8 total):
  #   A      - Learning rate [0,1]: Speed of expected value updating
  #   alpha  - Outcome sensitivity [0,2]: Curvature of utility function
  #   cons   - Choice consistency [0,5]: Inverse temperature for softmax
  #   lambda - Loss aversion [0,10]: Weighting of losses vs gains
  #   epP    - Positive perseverance [0,5]: Boost after positive outcomes
  #   epN    - Negative perseverance [0,5]: Boost after negative outcomes
  #   K      - Decay rate [0,1]: How fast perseverance decays over trials
  #   w      - Weight [0,1]: Balance between value (EV) and perseverance
  #            w=1: Pure value-based (reduces to PVL-Delta)
  #            w=0: Pure perseverance-based (ignores outcomes)
  #
  # Perseverance updating:
  #   Chosen deck:   pers(t+1) = pers(t) * K + boost
  #   Unchosen deck: pers(t+1) = pers(t) * K
  #   where boost = epP if outcome >= 0, else epN
  #
  # Choice rule:
  #   Combined = w * EV + (1-w) * perseverance
  #   P(deck d) = exp(cons * Combined_d) / sum(exp(cons * Combined_j))
  #
  # ===========================================================================

  # ---------------------------------------------------------------------------
  # Group-level priors (hyperpriors)
  # ---------------------------------------------------------------------------

  # Standard PVL-Delta parameters (same priors as pvl_delta.jags)
  mu_A ~ dbeta(2, 2)
  mu_alpha ~ dnorm(0.7, 4)T(0, 2)
  mu_cons ~ dnorm(1, 2)T(0, 5)
  mu_lambda ~ dnorm(2, 1)T(0, 10)

  # Perseverance boost parameters: Small positive values expected
  # Empirical range in Worthy et al. (2013): 0.1-1.5
  mu_epP ~ dnorm(0.5, 4)T(0, 5)
  mu_epN ~ dnorm(0.5, 4)T(0, 5)

  # Decay rate: Higher K means faster forgetting of perseverance
  # Typical range: 0.3-0.8 (moderate decay per trial)
  mu_K ~ dbeta(2, 2)

  # Weight between value and perseverance
  # Typical range: 0.5-0.9 (value usually dominates)
  mu_w ~ dbeta(2, 2)

  # Group-level standard deviations
  sigma_A ~ dunif(0, 0.5)
  sigma_alpha ~ dunif(0, 0.5)
  sigma_cons ~ dunif(0, 1.5)
  sigma_lambda ~ dunif(0, 2)
  sigma_epP ~ dunif(0, 0.8)
  sigma_epN ~ dunif(0, 0.8)
  sigma_K ~ dunif(0, 0.3)
  sigma_w ~ dunif(0, 0.3)

  # ---------------------------------------------------------------------------
  # Subject-level parameters
  # ---------------------------------------------------------------------------

  for (s in 1:N) {
    A[s] ~ dbeta(mu_A * (1/sigma_A^2 - 1), (1-mu_A) * (1/sigma_A^2 - 1))T(0.01, 0.99)
    alpha[s] ~ dnorm(mu_alpha, 1/sigma_alpha^2)T(0.01, 2)
    cons[s] ~ dnorm(mu_cons, 1/sigma_cons^2)T(0.01, 5)
    lambda[s] ~ dnorm(mu_lambda, 1/sigma_lambda^2)T(0.01, 10)
    epP[s] ~ dnorm(mu_epP, 1/sigma_epP^2)T(0.01, 5)
    epN[s] ~ dnorm(mu_epN, 1/sigma_epN^2)T(0.01, 5)
    K[s] ~ dbeta(mu_K * (1/sigma_K^2 - 1), (1-mu_K) * (1/sigma_K^2 - 1))T(0.01, 0.99)
    w[s] ~ dbeta(mu_w * (1/sigma_w^2 - 1), (1-mu_w) * (1/sigma_w^2 - 1))T(0.01, 0.99)
  }

  # ---------------------------------------------------------------------------
  # Trial-level model
  # ---------------------------------------------------------------------------

  for (s in 1:N) {

    # Initialize expected values and perseverance to zero
    for (d in 1:4) {
      ev[s, 1, d] <- 0
      pers[s, 1, d] <- 0
    }

    for (t in 1:Tsubj[s]) {

      # Compute choice probabilities using weighted combination
      for (d in 1:4) {
        combined[s, t, d] <- ev[s, t, d] * w[s] + pers[s, t, d] * (1 - w[s])
        v[s, t, d] <- cons[s] * combined[s, t, d]
        exp_v[s, t, d] <- exp(v[s, t, d])
      }

      for (d in 1:4) {
        p[s, t, d] <- exp_v[s, t, d] / sum(exp_v[s, t, ])
      }

      # Likelihood of observed choice
      choice[s, t] ~ dcat(p[s, t, ])

      # Compute utility (same as PVL-Delta)
      abs_outcome[s, t] <- abs(outcome[s, t]) + 0.001

      util[s, t] <- ifelse(outcome[s, t] >= 0,
                           pow(abs_outcome[s, t], alpha[s]),
                           -lambda[s] * pow(abs_outcome[s, t], alpha[s]))

      # Update expected value and perseverance for each deck
      for (d in 1:4) {

        # Expected value update (delta learning rule, chosen deck only)
        ev[s, t+1, d] <- equals(d, choice[s, t]) * (ev[s, t, d] + A[s] * (util[s, t] - ev[s, t, d])) +
                         (1 - equals(d, choice[s, t])) * ev[s, t, d]

        # Perseverance update: decay + boost for chosen deck
        pers_boost[s, t, d] <- equals(d, choice[s, t]) * ifelse(outcome[s, t] >= 0, epP[s], epN[s])
        pers[s, t+1, d] <- pers[s, t, d] * K[s] + pers_boost[s, t, d]
      }
    }
  }
}
