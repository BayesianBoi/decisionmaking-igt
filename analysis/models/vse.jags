model {
  # VSE Model for Iowa Gambling Task
  # Value + Sequential Exploration (PVL-Delta with perseverance)
  # Parameters: A (learning rate), alpha (outcome sensitivity),
  #             cons (choice consistency), lambda (loss aversion),
  #             epP (positive perseverance), epN (negative perseverance),
  #             K (perseverance decay), w (value weight)

  # Hyperpriors (group-level distributions)
  mu_A ~ dunif(0, 1)
  mu_alpha ~ dunif(0, 2)
  mu_cons ~ dunif(0, 5)
  mu_lambda ~ dunif(0, 10)
  mu_epP ~ dunif(0, 2)
  mu_epN ~ dunif(0, 2)
  mu_K ~ dunif(0, 1)
  mu_w ~ dunif(0, 1)

  sigma_A ~ dunif(0, 1)
  sigma_alpha ~ dunif(0, 2)
  sigma_cons ~ dunif(0, 5)
  sigma_lambda ~ dunif(0, 10)
  sigma_epP ~ dunif(0, 2)
  sigma_epN ~ dunif(0, 2)
  sigma_K ~ dunif(0, 1)
  sigma_w ~ dunif(0, 1)

  # Subject-level parameters
  for (s in 1:N) {
    # Raw parameters (unbounded)
    A_raw[s] ~ dnorm(mu_A, pow(sigma_A, -2))
    alpha_raw[s] ~ dnorm(mu_alpha, pow(sigma_alpha, -2))
    cons_raw[s] ~ dnorm(mu_cons, pow(sigma_cons, -2))
    lambda_raw[s] ~ dnorm(mu_lambda, pow(sigma_lambda, -2))
    epP_raw[s] ~ dnorm(mu_epP, pow(sigma_epP, -2))
    epN_raw[s] ~ dnorm(mu_epN, pow(sigma_epN, -2))
    K_raw[s] ~ dnorm(mu_K, pow(sigma_K, -2))
    w_raw[s] ~ dnorm(mu_w, pow(sigma_w, -2))

    # Constrained parameters
    A[s] <- phi(A_raw[s])           # learning rate: [0, 1]
    alpha[s] <- exp(alpha_raw[s])   # outcome sensitivity: (0, inf)
    cons[s] <- exp(cons_raw[s])     # choice consistency: (0, inf)
    lambda[s] <- exp(lambda_raw[s]) # loss aversion: (0, inf)
    epP[s] <- exp(epP_raw[s])       # positive perseverance: (0, inf)
    epN[s] <- exp(epN_raw[s])       # negative perseverance: (0, inf)
    K[s] <- phi(K_raw[s])           # perseverance decay: [0, 1]
    w[s] <- phi(w_raw[s])           # value weight: [0, 1]
  }

  # Trial-by-trial updates
  for (s in 1:N) {
    # Initialize expected values and perseverance
    for (d in 1:4) {
      ev[s, 1, d] <- 0
      pers[s, 1, d] <- 0
    }

    for (t in 1:Tsubj[s]) {
      # Sensitivity parameter (compute once per trial)
      sens[s, t] <- pow(3, cons[s]) - 1

      # Compute choice probabilities with softmax
      for (d in 1:4) {
        # Combined value: weighted sum of EV and perseverance
        combined_value[s, t, d] <- (ev[s, t, d] * w[s] + pers[s, t, d] * (1 - w[s])) * sens[s, t]

        # Exp-utility for each deck
        exp_util[s, t, d] <- exp(combined_value[s, t, d])
      }

      # Softmax probabilities
      for (d in 1:4) {
        p_choice[s, t, d] <- exp_util[s, t, d] / sum(exp_util[s, t, ])
      }

      # Observed choice likelihood
      choice[s, t] ~ dcat(p_choice[s, t, ])

      # Utility of observed outcome
      # Add small constant to avoid pow(0, alpha) issues
      util[s, t] <- ifelse(outcome[s, t] >= 0,
                           pow(abs(outcome[s, t]) + 0.001, alpha[s]),
                           -lambda[s] * pow(abs(outcome[s, t]) + 0.001, alpha[s]))

      # Update expected value for chosen deck (delta rule)
      # Update perseverance
      for (d in 1:4) {
        # EV update (only for chosen deck)
        ev[s, t+1, d] <- ifelse(d == choice[s, t],
                                ev[s, t, d] + A[s] * (util[s, t] - ev[s, t, d]),
                                ev[s, t, d])

        # Perseverance update
        pers[s, t+1, d] <- ifelse(d == choice[s, t],
                                  # Chosen deck: decay then add boost
                                  pers[s, t, d] * K[s] + ifelse(outcome[s, t] >= 0, epP[s], epN[s]),
                                  # Unchosen decks: just decay
                                  pers[s, t, d] * K[s])
      }
    }
  }
}
