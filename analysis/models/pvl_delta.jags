model {
  # ===========================================================================
  # PVL-Delta Model for Iowa Gambling Task
  # ===========================================================================
  #
  # Reference: Ahn, W.Y., Busemeyer, J.R., & Wagenmakers, E.J. (2008).
  #            Comparison of decision learning models using the generalization
  #            criterion method on the Iowa gambling task. Cognitive Science.
  #
  # Parameters:
  #   A      - Learning rate [0,1]: Speed of expected value updating
  #   alpha  - Outcome sensitivity [0,2]: Curvature of utility function
  #   cons   - Choice consistency [0,5]: Inverse temperature for softmax
  #   lambda - Loss aversion [0,10]: Weighting of losses vs gains
  #
  # Utility function (Prospect Theory):
  #   u(x) = x^alpha           if x >= 0  (gains)
  #   u(x) = -lambda * |x|^alpha  if x < 0   (losses)
  #
  # Learning rule (Rescorla-Wagner delta rule):
  #   EV(t+1) = EV(t) + A * [u(t) - EV(t)]
  #
  # Choice rule (softmax):
  #   P(deck d) = exp(cons * EV_d) / sum(exp(cons * EV_j))
  #
  # ===========================================================================

  # ---------------------------------------------------------------------------
  # Group-level priors (hyperpriors)
  # ---------------------------------------------------------------------------

  # Learning rate: Beta(2,2) prior centered at 0.5, allows full [0,1] range
  # Empirical range in IGT studies: 0.01-0.20 (Ahn et al., 2008)
  mu_A ~ dbeta(2, 2)

  # Outcome sensitivity: Normal prior centered at 0.7, typical range 0.3-1.0
  # Values < 1 indicate diminishing sensitivity (concave utility)
  mu_alpha ~ dnorm(0.7, 4)T(0, 2)

  # Choice consistency: Normal prior centered at 1.0, typical range 0.5-3.0
  # Higher values indicate more deterministic (less random) choice
  mu_cons ~ dnorm(1, 2)T(0, 5)

  # Loss aversion: Normal prior centered at 2.0, typical range 1.0-4.0
  # Values > 1 indicate losses loom larger than gains (Kahneman & Tversky)
  mu_lambda ~ dnorm(2, 1)T(0, 10)

  # Group-level standard deviations (individual differences)
  sigma_A ~ dunif(0, 0.5)
  sigma_alpha ~ dunif(0, 0.5)
  sigma_cons ~ dunif(0, 1.5)
  sigma_lambda ~ dunif(0, 2)

  # ---------------------------------------------------------------------------
  # Subject-level parameters
  # ---------------------------------------------------------------------------

  for (s in 1:N) {
    # Method of moments parameterization for beta distribution
    A[s] ~ dbeta(mu_A * (1/sigma_A^2 - 1), (1-mu_A) * (1/sigma_A^2 - 1))T(0.01, 0.99)

    # Truncated normal for unbounded parameters
    alpha[s] ~ dnorm(mu_alpha, 1/sigma_alpha^2)T(0.01, 2)
    cons[s] ~ dnorm(mu_cons, 1/sigma_cons^2)T(0.01, 5)
    lambda[s] ~ dnorm(mu_lambda, 1/sigma_lambda^2)T(0.01, 10)
  }

  # ---------------------------------------------------------------------------
  # Trial-level model
  # ---------------------------------------------------------------------------

  for (s in 1:N) {

    # Initialize expected values to zero for all four decks
    for (d in 1:4) {
      ev[s, 1, d] <- 0
    }

    for (t in 1:Tsubj[s]) {

      # Softmax choice probabilities
      for (d in 1:4) {
        v[s, t, d] <- cons[s] * ev[s, t, d]
        exp_v[s, t, d] <- exp(v[s, t, d])
      }

      for (d in 1:4) {
        p[s, t, d] <- exp_v[s, t, d] / sum(exp_v[s, t, ])
      }

      # Likelihood of observed choice
      choice[s, t] ~ dcat(p[s, t, ])

      # Compute utility of outcome
      # Small constant (0.001) prevents pow(0, alpha) numerical issues
      abs_outcome[s, t] <- abs(outcome[s, t]) + 0.001

      util[s, t] <- ifelse(outcome[s, t] >= 0,
                           pow(abs_outcome[s, t], alpha[s]),
                           -lambda[s] * pow(abs_outcome[s, t], alpha[s]))

      # Update expected value for chosen deck only (delta learning rule)
      for (d in 1:4) {
        ev[s, t+1, d] <- equals(d, choice[s, t]) * (ev[s, t, d] + A[s] * (util[s, t] - ev[s, t, d])) +
                         (1 - equals(d, choice[s, t])) * ev[s, t, d]
      }
    }
  }
}
