model {
  # ===========================================================================
  # EEF Model for Iowa Gambling Task - Clinical Populations
  # ===========================================================================
  #
  # Reference: Yang, X., Yao, S., Liu, T., & Fang, J. (2025). Exploitation
  #            and Exploration with Forgetting: A model for the IGT.
  #            Frontiers in Psychology, 16, 1510151.
  #
  # The EEF (Exploitation-Exploration with Forgetting) model separates
  # choice behavior into:
  #   1. Exploitation: Learning deck values from outcomes
  #   2. Exploration: Tracking recency of deck choices
  #   3. Forgetting: Memory decay applied to both components
  #
  # Key theoretical contribution: The same forgetting parameter (lambda)
  # governs both value decay and exploration recency, providing a unified
  # account of memory limitations in decision-making.
  #
  # Parameters (4 total):
  #   theta        - Outcome sensitivity [0,1]: Power function exponent
  #   lambda_forget - Forgetting rate [0,1]: Memory decay per trial
  #   phi          - Exploration incentive [-5,5]: Attraction to unchosen decks
  #   cons         - Choice consistency [0,5]: Inverse temperature
  #
  # Utility function (symmetric for gains and losses):
  #   V(t) = Gain^theta - Loss^theta
  #
  # Exploitation update (chosen deck):
  #   exploit(t+1) = (1 - lambda) * exploit(t) + V(t)
  #
  # Exploration update (unchosen decks):
  #   explore(t+1) = lambda * explore(t) + (1 - lambda) * phi
  #
  # Choice rule:
  #   P(deck d) = exp(cons * [exploit_d + explore_d]) / sum(...)
  #
  # ===========================================================================

  # ---------------------------------------------------------------------------
  # Group-level priors (hyperpriors)
  # ---------------------------------------------------------------------------

  # Outcome sensitivity: Based on Yang et al. (2025) Figure 9
  # Young adults: theta ~ 0.35, Older adults: theta ~ 0.30
  # Beta(1.5, 3) gives mean=0.33, mode=0.20
  mu_theta ~ dbeta(1.5, 3)

  # Forgetting rate: Primary parameter of interest for clinical comparison
  # Yang et al. (2025): Young adults ~ 0.46, Older adults ~ 0.54
  # Beta(2, 3) gives mean=0.40, mode=0.33
  # Hypothesis: Substance users will show higher lambda than healthy controls
  mu_lambda_forget ~ dbeta(2, 3)

  # Exploration incentive: Attraction toward unchosen decks
  # Yang et al. (2025): Young ~ 0.69, Old ~ 0.50 (both positive)
  # Normal(0, 2) allows negative values for exploration aversion
  mu_phi ~ dnorm(0, 2)T(-5, 5)

  # Choice consistency: Standard IGT range (Ahn et al., 2014)
  mu_cons ~ dnorm(1, 2)T(0, 5)

  # Group-level standard deviations (constrained for small clinical samples)
  sigma_theta ~ dunif(0, 0.3)
  sigma_lambda_forget ~ dunif(0, 0.3)
  sigma_phi ~ dunif(0, 2)
  sigma_cons ~ dunif(0, 1.5)

  # ---------------------------------------------------------------------------
  # Subject-level parameters
  # ---------------------------------------------------------------------------

  for (s in 1:N) {

    # Method of moments parameterization for bounded [0,1] parameters
    theta[s] ~ dbeta(
      mu_theta * (1/sigma_theta^2 - 1),
      (1 - mu_theta) * (1/sigma_theta^2 - 1)
    )T(0.01, 0.99)

    lambda_forget[s] ~ dbeta(
      mu_lambda_forget * (1/sigma_lambda_forget^2 - 1),
      (1 - mu_lambda_forget) * (1/sigma_lambda_forget^2 - 1)
    )T(0.01, 0.99)

    # Truncated normal for exploration and consistency
    phi[s] ~ dnorm(mu_phi, 1/sigma_phi^2)T(-5, 5)
    cons[s] ~ dnorm(mu_cons, 1/sigma_cons^2)T(0.01, 5)
  }

  # ---------------------------------------------------------------------------
  # Trial-level model
  # ---------------------------------------------------------------------------

  for (s in 1:N) {

    # Initialize exploitation weights from first-choice data (w_ini)
    # Initialize exploration weights to zero
    for (d in 1:4) {
      exploit[s, 1, d] <- w_ini[d]
      explore[s, 1, d] <- 0
    }

    for (t in 1:Tsubj[s]) {

      # Compute choice probabilities (softmax over combined weights)
      for (d in 1:4) {
        weight[s, t, d] <- exploit[s, t, d] + explore[s, t, d]
        v[s, t, d] <- cons[s] * weight[s, t, d]
        exp_v[s, t, d] <- exp(v[s, t, d])
      }

      for (d in 1:4) {
        p[s, t, d] <- exp_v[s, t, d] / sum(exp_v[s, t, ])
      }

      # Likelihood of observed choice
      choice[s, t] ~ dcat(p[s, t, ])

      # Compute utility (symmetric power function, no loss aversion)
      gain[s, t] <- max(outcome[s, t], 0)
      loss[s, t] <- max(-outcome[s, t], 0)

      # Small constant prevents pow(0, theta) numerical issues
      util[s, t] <- pow(gain[s, t] + 0.001, theta[s]) -
                    pow(loss[s, t] + 0.001, theta[s])

      # Exploitation update: decay old value, add new utility for chosen deck
      for (d in 1:4) {
        exploit[s, t+1, d] <- equals(d, choice[s, t]) *
                              ((1 - lambda_forget[s]) * exploit[s, t, d] + util[s, t]) +
                              (1 - equals(d, choice[s, t])) *
                              ((1 - lambda_forget[s]) * exploit[s, t, d])
      }

      # Exploration update: chosen deck resets, unchosen approach phi
      for (d in 1:4) {
        explore[s, t+1, d] <- equals(d, choice[s, t]) * 0 +
                              (1 - equals(d, choice[s, t])) *
                              (lambda_forget[s] * explore[s, t, d] +
                               (1 - lambda_forget[s]) * phi[s])
      }
    }
  }
}
