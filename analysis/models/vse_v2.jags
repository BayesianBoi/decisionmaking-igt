model {
  # VSE Model for Iowa Gambling Task
  # Value + Sequential Exploration (PVL-Delta with perseverance)
  # Fixed version with better numerical stability

  # Hyperpriors
  mu_A ~ dbeta(2, 2)
  mu_alpha ~ dnorm(0.5, 1)T(0,)
  mu_cons ~ dnorm(1, 1)T(0,)
  mu_lambda ~ dnorm(1, 1)T(0,)
  mu_epP ~ dnorm(0.5, 1)T(0,)
  mu_epN ~ dnorm(0.5, 1)T(0,)
  mu_K ~ dbeta(2, 2)
  mu_w ~ dbeta(2, 2)

  sigma_A ~ dunif(0, 0.5)
  sigma_alpha ~ dunif(0, 1)
  sigma_cons ~ dunif(0, 2)
  sigma_lambda ~ dunif(0, 3)
  sigma_epP ~ dunif(0, 1)
  sigma_epN ~ dunif(0, 1)
  sigma_K ~ dunif(0, 0.5)
  sigma_w ~ dunif(0, 0.5)

  # Subject-level parameters with truncation
  for (s in 1:N) {
    A[s] ~ dbeta(mu_A * (1/sigma_A^2 - 1), (1-mu_A) * (1/sigma_A^2 - 1))T(0.01, 0.99)
    alpha[s] ~ dnorm(mu_alpha, 1/sigma_alpha^2)T(0.01, 2)
    cons[s] ~ dnorm(mu_cons, 1/sigma_cons^2)T(0.01, 5)
    lambda[s] ~ dnorm(mu_lambda, 1/sigma_lambda^2)T(0.01, 10)
    epP[s] ~ dnorm(mu_epP, 1/sigma_epP^2)T(0.01, 5)
    epN[s] ~ dnorm(mu_epN, 1/sigma_epN^2)T(0.01, 5)
    K[s] ~ dbeta(mu_K * (1/sigma_K^2 - 1), (1-mu_K) * (1/sigma_K^2 - 1))T(0.01, 0.99)
    w[s] ~ dbeta(mu_w * (1/sigma_w^2 - 1), (1-mu_w) * (1/sigma_w^2 - 1))T(0.01, 0.99)
  }

  # Trial-by-trial updates
  for (s in 1:N) {
    # Initialize expected values and perseverance
    for (d in 1:4) {
      ev[s, 1, d] <- 0
      pers[s, 1, d] <- 0
    }

    for (t in 1:Tsubj[s]) {
      # Compute choice probabilities with softmax
      for (d in 1:4) {
        # Combined value: weighted sum of EV and perseverance
        combined[s, t, d] <- ev[s, t, d] * w[s] + pers[s, t, d] * (1 - w[s])
        v[s, t, d] <- cons[s] * combined[s, t, d]
        exp_v[s, t, d] <- exp(v[s, t, d])
      }

      # Softmax probabilities
      for (d in 1:4) {
        p[s, t, d] <- exp_v[s, t, d] / sum(exp_v[s, t, ])
      }

      # Observed choice
      choice[s, t] ~ dcat(p[s, t, ])

      # Compute utility
      abs_outcome[s, t] <- abs(outcome[s, t]) + 0.001

      util[s, t] <- ifelse(outcome[s, t] >= 0,
                           pow(abs_outcome[s, t], alpha[s]),
                           -lambda[s] * pow(abs_outcome[s, t], alpha[s]))

      # Update expected value and perseverance for each deck
      for (d in 1:4) {
        # EV update (only for chosen deck)
        ev[s, t+1, d] <- equals(d, choice[s, t]) * (ev[s, t, d] + A[s] * (util[s, t] - ev[s, t, d])) +
                         (1 - equals(d, choice[s, t])) * ev[s, t, d]

        # Perseverance update
        # Chosen deck: decay then add boost based on outcome valence
        # Unchosen decks: just decay
        pers_boost[s, t, d] <- equals(d, choice[s, t]) * ifelse(outcome[s, t] >= 0, epP[s], epN[s])
        pers[s, t+1, d] <- pers[s, t, d] * K[s] + pers_boost[s, t, d]
      }
    }
  }
}
