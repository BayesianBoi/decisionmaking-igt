model {
  # VSE Model for Iowa Gambling Task
  # Value + Sequential Exploration (Worthy et al., 2013)
  #
  # Extends PVL-Delta with sequential exploration (perseverance) mechanisms:
  # - A, alpha, cons, lambda: Same as PVL-Delta
  # - epP: Perseverance boost after positive outcomes [0, 5]
  # - epN: Perseverance boost after negative outcomes [0, 5]
  # - K: Perseverance decay rate [0, 1] - higher K = faster decay
  # - w: Weight between value and perseverance [0, 1]
  #   w=1: Pure value-based learning (reduces to PVL-Delta)
  #   w=0: Pure perseverance-based choice
  #
  # Perseverance updating:
  # - pers[t+1] = pers[t] * K + boost (chosen deck)
  # - pers[t+1] = pers[t] * K (unchosen decks)
  # - boost = epP if outcome >= 0, else epN
  #
  # Choice: softmax(cons * [EV*w + perseverance*(1-w)])
  #
  # This v2 version uses improved numerical stability with bounded parameters

  # Hyperpriors (weakly informative based on IGT literature)
  # Base PVL-Delta parameters (same as pvl_delta_v2)
  mu_A ~ dbeta(2, 2)
  mu_alpha ~ dnorm(0.7, 4)T(0, 2)
  mu_cons ~ dnorm(1, 2)T(0, 5)
  mu_lambda ~ dnorm(2, 1)T(0, 10)

  # VSE-specific parameters
  # epP/epN (perseverance boosts): Typically small values 0-2
  # Using precision=4 -> SD=0.5
  mu_epP ~ dnorm(0.5, 4)T(0, 5)
  mu_epN ~ dnorm(0.5, 4)T(0, 5)

  # K (decay rate): Higher values mean faster decay, typically 0.3-0.7
  mu_K ~ dbeta(2, 2)

  # w (weight): Balance between value and perseverance, typically 0.5-0.8
  mu_w ~ dbeta(2, 2)

  # Group-level variability
  sigma_A ~ dunif(0, 0.5)
  sigma_alpha ~ dunif(0, 0.5)
  sigma_cons ~ dunif(0, 1.5)
  sigma_lambda ~ dunif(0, 2)
  sigma_epP ~ dunif(0, 0.8)
  sigma_epN ~ dunif(0, 0.8)
  sigma_K ~ dunif(0, 0.3)
  sigma_w ~ dunif(0, 0.3)

  # Subject-level parameters with truncation
  for (s in 1:N) {
    A[s] ~ dbeta(mu_A * (1/sigma_A^2 - 1), (1-mu_A) * (1/sigma_A^2 - 1))T(0.01, 0.99)
    alpha[s] ~ dnorm(mu_alpha, 1/sigma_alpha^2)T(0.01, 2)
    cons[s] ~ dnorm(mu_cons, 1/sigma_cons^2)T(0.01, 5)
    lambda[s] ~ dnorm(mu_lambda, 1/sigma_lambda^2)T(0.01, 10)
    epP[s] ~ dnorm(mu_epP, 1/sigma_epP^2)T(0.01, 5)
    epN[s] ~ dnorm(mu_epN, 1/sigma_epN^2)T(0.01, 5)
    K[s] ~ dbeta(mu_K * (1/sigma_K^2 - 1), (1-mu_K) * (1/sigma_K^2 - 1))T(0.01, 0.99)
    w[s] ~ dbeta(mu_w * (1/sigma_w^2 - 1), (1-mu_w) * (1/sigma_w^2 - 1))T(0.01, 0.99)
  }

  # Trial-by-trial updates
  for (s in 1:N) {
    # Initialize expected values and perseverance
    for (d in 1:4) {
      ev[s, 1, d] <- 0
      pers[s, 1, d] <- 0
    }

    for (t in 1:Tsubj[s]) {
      # Compute choice probabilities with softmax
      for (d in 1:4) {
        # Combined value: weighted sum of EV and perseverance
        combined[s, t, d] <- ev[s, t, d] * w[s] + pers[s, t, d] * (1 - w[s])
        v[s, t, d] <- cons[s] * combined[s, t, d]
        exp_v[s, t, d] <- exp(v[s, t, d])
      }

      # Softmax probabilities
      for (d in 1:4) {
        p[s, t, d] <- exp_v[s, t, d] / sum(exp_v[s, t, ])
      }

      # Observed choice
      choice[s, t] ~ dcat(p[s, t, ])

      # Compute utility
      abs_outcome[s, t] <- abs(outcome[s, t]) + 0.001

      util[s, t] <- ifelse(outcome[s, t] >= 0,
                           pow(abs_outcome[s, t], alpha[s]),
                           -lambda[s] * pow(abs_outcome[s, t], alpha[s]))

      # Update expected value and perseverance for each deck
      for (d in 1:4) {
        # EV update (only for chosen deck)
        ev[s, t+1, d] <- equals(d, choice[s, t]) * (ev[s, t, d] + A[s] * (util[s, t] - ev[s, t, d])) +
                         (1 - equals(d, choice[s, t])) * ev[s, t, d]

        # Perseverance update
        # Chosen deck: decay then add boost based on outcome valence
        # Unchosen decks: just decay
        pers_boost[s, t, d] <- equals(d, choice[s, t]) * ifelse(outcome[s, t] >= 0, epP[s], epN[s])
        pers[s, t+1, d] <- pers[s, t, d] * K[s] + pers_boost[s, t, d]
      }
    }
  }
}
